# -*- coding: utf-8 -*-
"""ML_hw6_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E8bQqSCsTTatDXKHz-o3p3oDGv6JxVPy

<h2>PCA with just scaling</h2>

4a
"""

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns
from pathlib import Path 
import os 
import string 
import math

from google.colab import drive
drive.mount('/content/drive')

path = Path('/content/drive/My Drive/ml_hw6_q4/places.txt')

cd /content/drive/My Drive/ml_hw6_q4

data = pd.read_csv('places.txt',delim_whitespace=True,na_values='?')
table = data[['Climate', 'HousingCost', 'HlthCare', 'Crime', 'Transp', 'Educ', 'Arts','Recreat', 'Econ']]

"""4b"""

table = np.log10(table)
table.head(5)

"""4c"""

mean = np.mean(table,axis = 0)
print(mean)

table = table - mean
print(np.mean(table,axis = 0))

from sklearn.decomposition import PCA
pca = PCA()
pca.fit(table)
print(pca.components_.shape)

"""4d and 4e"""

pa1 = pca.components_[0]
pa2 = pca.components_[1]
print(f"First principal axis = {pa1}")
print(f"Second principal axis = {pa2}")

projected = pca.fit_transform(table)
print(f"First principal component = \n {projected[:, 0]}")
print(f"Second principal component = \n {projected[:, 1]}")

plt.scatter(projected[:, 0], projected[:, 1])
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Projection on two principal components')
plt.show()

PC1_outliers = np.where(projected[:, 0]>1.5)
PC1_outliers = np.squeeze(PC1_outliers)
print(PC1_outliers)

PC2_outliers = np.where(projected[:, 1]>0.63)
PC2_outliers = np.squeeze(PC2_outliers)
print(PC2_outliers)

"""Outliers observed from the above plot"""

outliers = []
outliers.append(data.index[212])
outliers.append(data.index[50])
outliers.append(data.index[119])
outliers.append(data.index[194])
print(outliers)

eigenvalues = pca.explained_variance_
print(eigenvalues)

from numpy import diag
sq_evalues = np.sqrt(eigenvalues)
evalues_diag = diag(sq_evalues)
cc = np.dot(pca.components_.T, evalues_diag)
print(cc.T[0])
print(cc.T[1])

"""We are only interested in the top correlations with the first and second principal components, which means that we should look at the first column of cc and select variables with highest absolute values.

1. For the first component:
> From cc.T[0] we can see that PC1 appears to correlate the most with arts feature with a correlation value of 0.537

2. For the second component:
> From cc.T[1] we can see that PC2 appears to correlate the most with Healthcare feature with a correlation value of 0.1939
"""



"""<h2>4f. PCA with standardizing</h2>"""

data = pd.read_csv('places.txt',delim_whitespace=True,na_values='?')
table = data[['Climate', 'HousingCost', 'HlthCare', 'Crime', 'Transp', 'Educ', 'Arts','Recreat', 'Econ']]

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(table)
mean = scaler.mean_
table = scaler.transform(table)

mean = np.mean(table,axis = 0)
print(mean)

std = np.std(table,axis = 0)
print(std)

from sklearn.decomposition import PCA
pca = PCA()
pca.fit(table)
print(pca.components_.shape)

pa1 = pca.components_[0]
pa2 = pca.components_[1]
print(f"First principal axis = {pa1}")
print(f"Second principal axis = {pa2}")

projected = pca.fit_transform(table)
print(f"First principal component = \n {projected[:, 0]}")
print(f"Second principal component = \n {projected[:, 1]}")

plt.scatter(projected[:, 0], projected[:, 1])
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Projection on two principal components')
plt.show()

"""Outliers observed from the above plot"""

PC1_outliers = np.where(projected[:, 0]>12)
PC1_outliers = np.squeeze(PC1_outliers)
print(PC1_outliers)

PC2_outliers = np.where(projected[:, 1]<-3)
PC2_outliers = np.squeeze(PC2_outliers)
print(PC2_outliers)

outliers = []
outliers.append(data.index[212])
outliers.append(data.index[64])
outliers.append(data.index[233])
print(outliers)

eigenvalues = pca.explained_variance_
print(eigenvalues)

from numpy import diag
sq_evalues = np.sqrt(eigenvalues)
evalues_diag = diag(sq_evalues)
cc = np.dot(pca.components_.T, evalues_diag)
print(cc.T[0])
print(cc.T[1])

"""We are only interested in the top correlations with the first and second principal components, which means that we should look at the first column of cc and select variables with highest absolute values.

1. For the first component:
> From cc.T[0] we can see that PC1 appears to correlate the most with arts feature with a correlation value of 0.856

2. For the second component:
> From cc.T[1] we can see that PC2 appears to correlate the most with Education feature with a correlation value of 0.533

The consistent outlier found in cases of scaling(mean = 0) and standardizing(mean=0, var = 1) is sample number 212 which is New York, NY

> Principal component analysis will tend to give more emphasis to those variables that have higher variances than to those variables that have lower variances. In effect the results of the analysis will depend on the units of measurement used to measure each variable. That would imply that a principal component analysis should only be used with the raw data if all variables have the same units of measure. And even in this case, only if you wish to give those variables which have higher variances more weight in the analysis. 

> From the above reasoning we can conclude that it is necessary to standardize the variables before performing PCA. From the above solution we can see that the outliers are different in both the cases, except for the outlier 212(New York).

> From the above plots we see that the results for standardizing and not standardizing vary. PC1 and PC2 value are different which changes the plot this inturn changes the outliers.
"""

