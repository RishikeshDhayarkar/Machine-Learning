{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_ML_assign_1_q4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oArdxxj7Nmq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvXVqBTxOD41",
        "colab_type": "code",
        "outputId": "ed3c40c0-f905-4633-c25a-d13a3864974b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install num2words"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.6/dist-packages (0.5.10)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW3bawkMOLMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import num2words\n",
        "from num2words import num2words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFhxuuBEOP1q",
        "colab_type": "code",
        "outputId": "a8818fde-c41e-4978-d34a-e7afccdfa150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zG59sMsOSyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = Path('/content/drive/My Drive/assignment_1_mod')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqpSvqGZOekX",
        "colab_type": "code",
        "outputId": "a4274705-98e1-4928-ff07-c10baf3f926c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/assignment_1_mod"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/assignment_1_mod\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PWG0zxaOgeC",
        "colab_type": "code",
        "outputId": "97854e13-f332-42ac-c211-a7fa1022a1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset = os.listdir(path)\n",
        "print(dataset)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['d1.txt', 'd3.txt', 'd2.txt', 'd4.txt', 'd5.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AzMDq7bOmNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_document(id):\n",
        "    print(dataset[id])\n",
        "    file = open(dataset[id], 'r')\n",
        "    text = file.read().strip()\n",
        "    file.close()\n",
        "    print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhDXur5yO_Ts",
        "colab_type": "text"
      },
      "source": [
        "<h2>Preprocessing of dataset</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpUnW83cO5Sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lowercase(inputs):\n",
        "    return np.char.lower(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA89eMgEPbPe",
        "colab_type": "code",
        "outputId": "dab935c3-e7c0-47c7-aef2-179e645ff223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj0ohm-4PeBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stopwords_remover(inputs):\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = word_tokenize(str(inputs))\n",
        "    modified_text = \"\"\n",
        "    for word in words:\n",
        "        if word not in stop_words and len(word) > 1:\n",
        "            modified_text = modified_text + \" \" + word\n",
        "    return modified_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq1jLtKYQPhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def punctuation_remover(inputs):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in range(len(symbols)):\n",
        "        inputs = np.char.replace(inputs, symbols[i], ' ')\n",
        "        inputs = np.char.replace(inputs, \"  \", \" \")\n",
        "    inputs = np.char.replace(inputs, ',', '')\n",
        "    inputs = np.char.replace(inputs, \"'\", \"\")\n",
        "    return inputs   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYsTL8gddDH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemming(inputs):\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = word_tokenize(str(inputs))\n",
        "    modified_text = \"\"\n",
        "    for word in tokens:\n",
        "        modified_text = modified_text + \" \" + stemmer.stem(word)\n",
        "    return modified_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p46OUcXIdD1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_numbers(inputs):\n",
        "    tokens = word_tokenize(str(inputs))\n",
        "    modified_text = \"\"\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            word = num2words(int(word))\n",
        "        except:\n",
        "            a = 0\n",
        "        modified_text = modified_text + \" \" + word\n",
        "    modified_text = np.char.replace(modified_text, \"-\", \" \")\n",
        "    return modified_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAol0ZEtfqwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "    text = lowercase(text)\n",
        "    text = punctuation_remover(text) \n",
        "    text = stopwords_remover(text)\n",
        "    text = convert_numbers(text)\n",
        "    text = stemming(text)\n",
        "    text = punctuation_remover(text)\n",
        "    text = convert_numbers(text)\n",
        "    text = stemming(text) \n",
        "    text = punctuation_remover(text) \n",
        "    text = stopwords_remover(text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5EimKIsj459",
        "colab_type": "code",
        "outputId": "2a653968-bdc8-48d6-cf04-43d30c791b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNK3weKKj6H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_text = []\n",
        "for i in dataset:\n",
        "    file = open(i, 'r', errors='ignore')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    processed_text.append(word_tokenize(str(preprocess(text))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqrcj9YpjtXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDqYT-VwU-NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = len(dataset)\n",
        "# N = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6FP1kiBTzwA",
        "colab_type": "text"
      },
      "source": [
        "'processed_text' contains a list of lists of the words occurring in each document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk62Ph4LUDcj",
        "colab_type": "text"
      },
      "source": [
        "Creating document frequency(nj in idf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3FC5sI2kG9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nj = {} \n",
        "# nj is a dictionary that is of the format key : {}\n",
        "# key is the word from a list and {} is a set containing all the documents in which 'key' is occuring\n",
        "for i in range(N):\n",
        "    tokens = processed_text[i]\n",
        "    for word in tokens:\n",
        "        try:\n",
        "          # when the set contains some element in it add a new element to the set\n",
        "            nj[word].add(i) \n",
        "        except:\n",
        "          # when the set is empty\n",
        "            nj[word] = {i}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtu9RefGU8f6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# counting the number of elements present in the set part if nj dictionary. \n",
        "# This gives the number of documents in which a word is occurring.\n",
        "for i in nj:\n",
        "    nj[i] = len(nj[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWKC3E0AYIrc",
        "colab_type": "text"
      },
      "source": [
        "Now nj contains a dictionary representing a word and the number of documents in which that word is used.\n",
        "Total number of elements present in nj dictionary should give the entire set of words that are used in our dataset.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXqWZ77jVEK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(nj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsLQLFSFY222",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a list of vocab words of our dataset\n",
        "total_vocab = [x for x in nj]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU19p5sRZE2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function accesses the nj dictionary and gives the document count for the word passed into the function\n",
        "def document_frequency(word):\n",
        "    counts = 0\n",
        "    try:\n",
        "        counts = nj[word]\n",
        "    except:\n",
        "        pass\n",
        "    return counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u8XhefyZhCp",
        "colab_type": "text"
      },
      "source": [
        "<h2>Calculating tf-idf</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D18-1QiyZQSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = 0\n",
        "tf_idf = {}\n",
        "# tf_idf is a dictionary that is of the form (document number, token) : tf-idf value\n",
        "for i in range(N):\n",
        "    tokens = processed_text[i]\n",
        "    # A Counter is a container that keeps track of how many times equivalent values are added. The method 'Counter' returns a dictionary\n",
        "    # with key value as token and value as counts\n",
        "    counter = Counter(tokens)\n",
        "    words_count = len(tokens)\n",
        "    \n",
        "    for token in np.unique(tokens):\n",
        "        tf = counter[token]/words_count\n",
        "        df = document_frequency(token)\n",
        "        idf = np.log((N+1)/(df+1))\n",
        "  \n",
        "        tf_idf[doc, token] = tf*idf\n",
        "\n",
        "    doc += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4agOTGlqj9D1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5slTp8Vcmob",
        "colab_type": "text"
      },
      "source": [
        "<h2>Finding similarity</h2>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaxCc-YObLw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to calculate the cosine similarity \n",
        "def cosine_sim(x,y):\n",
        "    similarity = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
        "    return similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPibYIPPc76h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  'tf_idf_matrix' is a matrix initialized to zeros. The matrix tf_idf_matrix gives a tabular represenation of tf_idf values of all vocab words(columns)\n",
        "#  arranged according to the documents to which they belong to. \n",
        "# Each row of tf_idf_matrix represents a document and each column of tf_idf_matrix represents a word of the vocab. \n",
        "\n",
        "tf_idf_matrix = np.zeros((N, vocab_size))\n",
        "for i in tf_idf:\n",
        "    try:\n",
        "        # extract the token from the key value of tf_idf dictionary and put it into the variable cell.\n",
        "        cell = total_vocab.index(i[1])\n",
        "        # Inserting the calculated tf_idf value of the token into a position in the D matrix.\n",
        "        tf_idf_matrix[i[0]][cell] = tf_idf[i]\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIgjDgUYkBQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR7UrSQKfWOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for performing tf-idf calculations on the query document. Same as that of tf_idf code that's written above. \n",
        "def query_document_vector(tokens):\n",
        "    query_tf_idf_matrix = np.zeros((len(total_vocab)))\n",
        "    counter = Counter(tokens)\n",
        "    words_count = len(tokens)\n",
        "    \n",
        "    for token in np.unique(tokens):\n",
        "        tf = counter[token]/words_count\n",
        "        df = document_frequency(token)\n",
        "        idf = math.log((N+1)/(df+1))\n",
        "\n",
        "        try:\n",
        "            cell = total_vocab.index(token)\n",
        "            query_tf_idf_matrix[cell] = tf*idf\n",
        "        except:\n",
        "            pass\n",
        "    return query_tf_idf_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRllkPxthZBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that generates a list of cosine similarity values for the query\n",
        "def cosine_similarity(query):\n",
        "    preprocessed_query_document = preprocess(query)\n",
        "    tokens = word_tokenize(str(preprocessed_query_document))\n",
        "    \n",
        "    print(\"\\nGiven Query:\", query)\n",
        "    print(\"\")\n",
        "    \n",
        "    # A list for storing the cosine similarities generated between documents\n",
        "    cosine_values = []\n",
        "    # creating tf_idf vector for query document\n",
        "    query_td_idf_vector = query_document_vector(tokens)\n",
        "    \n",
        "    for xi in tf_idf_matrix:\n",
        "      # Calculating cosine similarities between (d1, d_query) to (d4, d_query)\n",
        "        cosine_values.append(cosine_sim(query_td_idf_vector, xi))\n",
        "      \n",
        "    # Sorting the 'cosine_values' list and returning it in the form of corresponding sorted indices using 'argsort()' function.\n",
        "    output = np.array(cosine_values).argsort()[::-1]\n",
        "    # 'output' is a list that contains the document numbers sorted(Descending) according to their cosine values.\n",
        "    #  \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZILXav6rYVL",
        "colab_type": "text"
      },
      "source": [
        "Instead of using the query document as a txt file I've used the contents of d_query.txt in my 'cosine_similarity' function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqKE3b0AjaB0",
        "colab_type": "code",
        "outputId": "fedcf7fc-6737-4600-8768-f94a074629c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "sorted_similarites = cosine_similarity(\"java coffee mocha\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Given Query: java coffee mocha\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD3cKyt4jg4f",
        "colab_type": "code",
        "outputId": "8643ed4f-1f7f-4e0d-830e-4e9e7b5fc510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "# Printing the document with maximum cosine similarity\n",
        "print_document(sorted_similarites[0])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "d4.txt\n",
            "Java coffee refers to coffee beans produced in the Indonesian island of Java.  The Indonesian phrase Kopi Java refers not only to the origin of the coffee, but is used to distinguish a style of strong, black, and very sweet coffee.  In some countries, Java can refer to coffee in general.\n",
            "\n",
            "The coffee is primarily grown on large estates that were built by the Dutch in the 18th century. The five largest estates are Blawan (also spelled Belawan or Blauan), Jampit (or Djampit), Pancoer (or Pancur), Kayumas and Tugosari, and they cover more than 4,000 hectares.\n",
            "\n",
            "These estates transport ripe cherries quickly to their mills after harvest. The pulp is then fermented and washed off, using the wet process, with rigorous quality control. This results in coffee with good, heavy body and a sweet overall impression. They are sometimes rustic in their flavor profiles, but display a lasting finish. At their best, they are smooth and supple and sometimes have a subtle herbaceous note in the after-taste.\n",
            "\n",
            "This coffee is prized as one component in the traditional \"Mocca Java\" blend, which pairs coffee from Yemen and Java. Certain estates age a portion of their coffee for up to five years, normally in large burlap sacks, which are regularly aired, dusted, and flipped. As they age, the beans turn from green to light brown, and their flavor gains strength while losing acidity. Aged coffees can display flavors ranging from cedar to spices such as cinnamon or clove, and often develop a thick, almost syrupy body. These aged coffees are called Old Government, Old Brown or Old Java.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QufAJmtMqbAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}